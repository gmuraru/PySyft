{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running this notebook, you should first run `./mnist_utils.py --option features --reduced 100 --binary` using the [mnist_utils.py from CrypTen](https://github.com/facebookresearch/CrypTen/blob/b1466440bde4db3e6e1fcb1740584d35a16eda9e/tutorials/mnist_utils.py) to prepare the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was '/home/youben/anaconda3/envs/pysyft-dev-py37/lib/python3.7/site-packages/tf_encrypted/operations/secure_random/secure_random_module_tf_1.15.0.so'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/youben/anaconda3/envs/pysyft-dev-py37/lib/python3.7/site-packages/tf_encrypted/session.py:24: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import crypten\n",
    "import syft\n",
    "from time import time\n",
    "from syft import WebsocketClientWorker\n",
    "from syft.frameworks.crypten.context import run_multiworkers\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.set_num_threads(1)\n",
    "hook = syft.TorchHook(torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an example network\n",
    "class ExampleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ExampleNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=5, padding=0)\n",
    "        self.fc1 = nn.Linear(16 * 12 * 12, 100)\n",
    "        self.fc2 = nn.Linear(100, 2)  # For binary classification, final layer needs only 2 outputs\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = F.relu(out)\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[%] Connecting to workers ...\n",
      "[+] Connected to workers\n",
      "[%] Sending labels and training data ...\n",
      "[+] Data ready\n"
     ]
    }
   ],
   "source": [
    "# Syft workers\n",
    "print(\"[%] Connecting to workers ...\")\n",
    "ALICE = syft.VirtualWorker(hook=hook, id=\"alice\")\n",
    "BOB = syft.VirtualWorker(hook=hook, id=\"bob\")\n",
    "print(\"[+] Connected to workers\")\n",
    "\n",
    "# Add crypten support to the worker\n",
    "ALICE.add_crypten_support()\n",
    "BOB.add_crypten_support()\n",
    "\n",
    "print(\"[%] Sending labels and training data ...\")\n",
    "# Prepare and send labels\n",
    "label_eye = torch.eye(2)\n",
    "labels = torch.load(\"/tmp/train_labels.pth\")\n",
    "labels = labels.long()\n",
    "labels_one_hot = label_eye[labels]\n",
    "labels_one_hot.tag(\"labels\")\n",
    "al_ptr = labels_one_hot.send(ALICE)\n",
    "bl_ptr = labels_one_hot.send(BOB)\n",
    "\n",
    "# Prepare and send training data\n",
    "alice_train = torch.load(\"/tmp/alice_train.pth\").tag(\"alice_train\")\n",
    "at_ptr = alice_train.send(ALICE)\n",
    "bob_train = torch.load(\"/tmp/bob_train.pth\").tag(\"bob_train\")\n",
    "bt_ptr = bob_train.send(BOB)\n",
    "\n",
    "print(\"[+] Data ready\")\n",
    "\n",
    "# Initialize model\n",
    "dummy_input = torch.empty(1, 1, 28, 28)\n",
    "pytorch_model = ExampleNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@run_multiworkers([ALICE, BOB], master_addr=\"127.0.0.1\", model=pytorch_model, dummy_input=dummy_input)\n",
    "def run_encrypted_training():\n",
    "    rank = crypten.communicator.get().get_rank()\n",
    "    \n",
    "    worker_id = syft.local_worker.rank_to_worker_id[rank]\n",
    "    worker = syft.local_worker.get_worker(worker_id)\n",
    "    labels_one_hot = worker.search(\"labels\")[0]\n",
    "\n",
    "    # Load data:\n",
    "    x_alice_enc = crypten.load(\"alice_train\", 0)\n",
    "    x_bob_enc = crypten.load(\"bob_train\", 1)\n",
    "\n",
    "    # Combine the feature sets: identical to Tutorial 3\n",
    "    x_combined_enc = crypten.cat([x_alice_enc, x_bob_enc], dim=2)\n",
    "\n",
    "    # Reshape to match the network architecture\n",
    "    x_combined_enc = x_combined_enc.unsqueeze(1)\n",
    "\n",
    "    # model is sent from the master worker\n",
    "    model.encrypt()\n",
    "    # Set train mode\n",
    "    model.train()\n",
    "\n",
    "    # Define a loss function\n",
    "    loss = crypten.nn.MSELoss()\n",
    "\n",
    "    # Define training parameters\n",
    "    learning_rate = 0.001\n",
    "    num_epochs = 2\n",
    "    batch_size = 10\n",
    "    num_batches = x_combined_enc.size(0) // batch_size\n",
    "\n",
    "    for i in range(num_epochs):\n",
    "        # Print once for readability\n",
    "        if rank == 0:\n",
    "            print(f\"Epoch {i} in progress:\")\n",
    "            pass\n",
    "\n",
    "        for batch in range(num_batches):\n",
    "            # define the start and end of the training mini-batch\n",
    "            start, end = batch * batch_size, (batch + 1) * batch_size\n",
    "\n",
    "            # construct AutogradCrypTensors out of training examples / labels\n",
    "            x_train = x_combined_enc[start:end]\n",
    "            y_batch = labels_one_hot[start:end]\n",
    "            y_train = crypten.cryptensor(y_batch, requires_grad=True)\n",
    "\n",
    "            # perform forward pass:\n",
    "            output = model(x_train)\n",
    "\n",
    "            loss_value = loss(output, y_train)\n",
    "\n",
    "            # set gradients to \"zero\"\n",
    "            model.zero_grad()\n",
    "\n",
    "            # perform backward pass:\n",
    "            loss_value.backward()\n",
    "\n",
    "            # update parameters\n",
    "            model.update_parameters(learning_rate)\n",
    "\n",
    "            # Print progress every batch:\n",
    "            batch_loss = loss_value.get_plain_text()\n",
    "            if rank == 0:\n",
    "                print(f\"\\tBatch {(batch + 1)} of {num_batches} Loss {batch_loss.item():.4f}\")\n",
    "\n",
    "    model.decrypt()\n",
    "    # printed contain all the printed strings during training\n",
    "    return printed, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[%] Starting computation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n",
      "/home/youben/anaconda3/envs/pysyft-dev-py37/lib/python3.7/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n",
      "/home/youben/anaconda3/envs/pysyft-dev-py37/lib/python3.7/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n",
      "/pytorch/aten/src/ATen/native/BinaryOps.cpp:66: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n",
      "/pytorch/aten/src/ATen/native/BinaryOps.cpp:66: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n",
      "/pytorch/aten/src/ATen/native/BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n",
      "/pytorch/aten/src/ATen/native/BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] run_encrypted_training() took 50s\n",
      "Epoch 0 in progress:\n",
      "\tBatch 1 of 10 Loss 0.4638\n",
      "\tBatch 2 of 10 Loss 0.4665\n",
      "\tBatch 3 of 10 Loss 0.4065\n",
      "\tBatch 4 of 10 Loss 0.3489\n",
      "\tBatch 5 of 10 Loss 0.3312\n",
      "\tBatch 6 of 10 Loss 0.2796\n",
      "\tBatch 7 of 10 Loss 0.2767\n",
      "\tBatch 8 of 10 Loss 0.2430\n",
      "\tBatch 9 of 10 Loss 0.2457\n",
      "\tBatch 10 of 10 Loss 0.2002\n",
      "Epoch 1 in progress:\n",
      "\tBatch 1 of 10 Loss 0.1624\n",
      "\tBatch 2 of 10 Loss 0.1516\n",
      "\tBatch 3 of 10 Loss 0.1549\n",
      "\tBatch 4 of 10 Loss 0.1922\n",
      "\tBatch 5 of 10 Loss 0.1319\n",
      "\tBatch 6 of 10 Loss 0.1634\n",
      "\tBatch 7 of 10 Loss 0.2243\n",
      "\tBatch 8 of 10 Loss 0.1454\n",
      "\tBatch 9 of 10 Loss 0.1717\n",
      "\tBatch 10 of 10 Loss 0.1334\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"[%] Starting computation\")\n",
    "func_ts = time()\n",
    "result = run_encrypted_training()\n",
    "func_te = time()\n",
    "print(f\"[+] run_encrypted_training() took {int(func_te - func_ts)}s\")\n",
    "printed = result[0][0]\n",
    "model = result[0][1]\n",
    "print(printed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<crypten.nn.module.Graph object at 0x7f5f91672e10>\n",
      "(Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
      "\t-> [PointerTensor | me:70320103064 -> alice:76402485570]\n",
      "\t-> [PointerTensor | me:99473637600 -> bob:24884125725]\n",
      "\t*crypto provider: cp*\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/youben/git-repo/PySyft/syft/frameworks/torch/tensors/interpreters/additive_shared.py:79: UserWarning: Use dtype instead of field\n",
      "  warnings.warn(\"Use dtype instead of field\")\n",
      "/home/youben/git-repo/PySyft/syft/frameworks/torch/tensors/interpreters/additive_shared.py:91: UserWarning: Default args selected\n",
      "  warnings.warn(\"Default args selected\")\n"
     ]
    }
   ],
   "source": [
    "cp = syft.VirtualWorker(hook=hook, id=\"cp\")\n",
    "model.fix_prec()\n",
    "model.share(ALICE, BOB, crypto_provider=cp)\n",
    "print(model)\n",
    "print(list(model.parameters())[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
